{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8n_dmTKO4_NC",
        "outputId": "7e85992a-c352-4342-ee0f-f29003dd5067"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May  1 09:12:34 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Connect to T4 GPU\n",
        "!nvidia-smi  # Verify GPU connection"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import psutil\n",
        "\n",
        "# Check memory usage\n",
        "memory_info = psutil.virtual_memory()\n",
        "print(f\"Total Memory: {memory_info.total / (1024 ** 2):.2f} MB\")\n",
        "print(f\"Available Memory: {memory_info.available / (1024 ** 2):.2f} MB\")\n",
        "print(f\"Used Memory: {memory_info.used / (1024 ** 2):.2f} MB\")\n",
        "print(f\"Memory Percentage: {memory_info.percent}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KJlCIuCPMri",
        "outputId": "93f78165-d0a3-4d2b-abc8-2d213ee0e3d3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Memory: 12977.95 MB\n",
            "Available Memory: 11640.71 MB\n",
            "Used Memory: 1031.46 MB\n",
            "Memory Percentage: 10.3%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jkpp4wlCCyPW",
        "outputId": "bc862408-155e-4f4f-bb17-05009c575d25"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.11.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy matplotlib seaborn scikit-learn tensorflow==2.15.0\n",
        "!pip install indic-nlp-library\n",
        "!pip install ipython\n",
        "!pip install xgboost\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8B489jEDtCI",
        "outputId": "d3b395c4-c118-47c1-9db6-421156f26cb6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.13.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.13.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.71.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, numpy, keras, ml-dtypes, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed keras-2.15.0 ml-dtypes-0.2.0 numpy-1.26.4 protobuf-4.25.7 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.5.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl.metadata (628 bytes)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from indic-nlp-library) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->indic-nlp-library) (2025.2)\n",
            "Requirement already satisfied: sphinx>=5.1.0 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (8.2.3)\n",
            "Requirement already satisfied: docutils>=0.19 in /usr/local/lib/python3.11/dist-packages (from sphinx-argparse->indic-nlp-library) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library) (1.17.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.0.0)\n",
            "Requirement already satisfied: Jinja2>=3.1 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.6)\n",
            "Requirement already satisfied: Pygments>=2.17 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.19.1)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.30.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.32.3)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.1.0)\n",
            "Requirement already satisfied: packaging>=23.0 in /usr/local/lib/python3.11/dist-packages (from sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.30.0->sphinx>=5.1.0->sphinx-argparse->indic-nlp-library) (2025.4.26)\n",
            "Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Downloading sphinx_argparse-0.5.2-py3-none-any.whl (12 kB)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: morfessor, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.92 morfessor-2.0.6 sphinx-argparse-0.5.2 sphinx-rtd-theme-3.0.2 sphinxcontrib-jquery-4.1\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython) (0.2.13)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jedi\n",
            "Successfully installed jedi-0.19.2\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from xgboost) (1.15.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ============================================\n",
        "#         Import Required Libraries\n",
        "# ============================================\n",
        "\n",
        "# After !pip installation restart the session\n",
        "import scipy\n",
        "import scipy.sparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
        "import joblib\n",
        "from sklearn import metrics\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from xgboost import XGBClassifier\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "from indicnlp.normalize.indic_normalize import DevanagariNormalizer\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "nM4M-M7oDPpy"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "#     ENHANCED MARATHI TEXT PROCESSOR\n",
        "# ======================================\n",
        "class MarathiTextProcessor:\n",
        "    def __init__(self, remove_nuktas=False, nasals_mode='do_nothing',\n",
        "                 normalize_chandras=False, normalize_vowel_endings=False):\n",
        "        self.normalizer = DevanagariNormalizer(\n",
        "            lang='mr',\n",
        "            remove_nuktas=remove_nuktas,\n",
        "            nasals_mode=nasals_mode,\n",
        "            do_normalize_chandras=normalize_chandras,\n",
        "            do_normalize_vowel_ending=normalize_vowel_endings\n",
        "        )\n",
        "        # Custom stop words to exclude\n",
        "        self.stop_words = {'à¤µà¤¾à¤šà¤¾ à¤¸à¤¤à¥à¤¯', 'à¤µà¥à¤¹à¤¾à¤¯à¤°à¤²', 'à¤¨à¤¾à¤¹à¥€', 'à¤¸à¥à¤•à¥à¤°à¤¿à¤ªà¥à¤Ÿà¥‡à¤¡', 'à¤¨à¤µà¥à¤¹à¤¤à¤¾','à¤¨à¤µ', 'à¤¯à¤°à¤²', 'à¤¸à¤¤', 'à¤¹à¤¤'}\n",
        "\n",
        "    def preprocess(self, text, remove_punctuation=True):\n",
        "        text = str(text)\n",
        "\n",
        "        # Step 1: Basic cleaning\n",
        "        if remove_punctuation:\n",
        "            text = re.sub(r'[^\\u0900-\\u097F\\s]', '', text)\n",
        "        else:\n",
        "            text = re.sub(r'[^\\u0900-\\u097F\\u0964\\u0965\\s]', '', text)\n",
        "\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "        # Step 2: Script normalization\n",
        "        text = self.normalizer.normalize(text)\n",
        "\n",
        "        # Step 3: Tokenization and stop word removal\n",
        "        tokens = indic_tokenize.trivial_tokenize(text)\n",
        "        filtered_tokens = [\n",
        "            token for token in tokens\n",
        "            if token not in self.stop_words\n",
        "        ]\n",
        "\n",
        "        return ' '.join(filtered_tokens)"
      ],
      "metadata": {
        "id": "Cyh1poUqRYCS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "#       DATA LOADING FUNCTIONS\n",
        "# ======================================\n",
        "def load_and_preview_examples():\n",
        "    \"\"\"Load and preview the Marathi news datasets\"\"\"\n",
        "    lokmat = pd.read_csv('lokmat_marathi_articles.csv').assign(source='Lokmat', label=1)\n",
        "    ndtv = pd.read_csv('ndtv_marathi_articles.csv').assign(source='NDTV', label=1)\n",
        "    factcrescendo = pd.read_csv('factcrescendo_marathi_articles.csv').assign(source='FactCrescendo', label=0)\n",
        "\n",
        "    # Detect topics\n",
        "    def detect_topic(text):\n",
        "        text = str(text).lower()\n",
        "        topic_keywords = {\n",
        "            'Politics': ['à¤°à¤¾à¤œà¤•à¤¾à¤°à¤£', 'à¤®à¤‚à¤¤à¥à¤°à¥€', 'à¤ªà¤•à¥à¤·', 'à¤¨à¤¿à¤µà¤¡à¤£à¥‚à¤•'],\n",
        "            'Health': ['à¤†à¤°à¥‹à¤—à¥à¤¯', 'à¤µà¥ˆà¤¦à¥à¤¯à¤•à¥€à¤¯', 'à¤°à¥‹à¤—', 'à¤µà¤¿à¤·à¤¾à¤£à¥‚'],\n",
        "            'Technology': ['à¤¤à¤‚à¤¤à¥à¤°à¤œà¥à¤à¤¾à¤¨', 'à¤®à¥‹à¤¬à¤¾à¤‡à¤²', 'à¤…à¥…à¤ª', 'à¤¡à¤¿à¤œà¤¿à¤Ÿà¤²'],\n",
        "            'Sports': ['à¤•à¥à¤°à¤¿à¤•à¥‡à¤Ÿ', 'à¤«à¥à¤Ÿà¤¬à¥‰à¤²', 'à¤¸à¥à¤ªà¤°à¥à¤§à¤¾', 'à¤ªà¤¥à¤•']\n",
        "        }\n",
        "        for topic, keywords in topic_keywords.items():\n",
        "            if any(keyword in text for keyword in keywords):\n",
        "                return topic\n",
        "        return 'Others'\n",
        "\n",
        "    for df in [lokmat, ndtv, factcrescendo]:\n",
        "        df['topic'] = df['Title'].apply(detect_topic)\n",
        "\n",
        "    # Combine and return\n",
        "    positive = pd.concat([lokmat, ndtv], ignore_index=True)\n",
        "    negative = factcrescendo.copy()\n",
        "\n",
        "    print(\"âœ… Positive Samples:\", len(positive), \"| Sources:\", positive['source'].unique())\n",
        "    print(\"âŒ Negative Samples:\", len(negative), \"| Sources:\", negative['source'].unique())\n",
        "    return positive, negative\n",
        "\n",
        "def prepare_datasets(positive_data, negative_data):\n",
        "    \"\"\"Prepare datasets with metadata\"\"\"\n",
        "    data_all = pd.concat([positive_data, negative_data], ignore_index=True)\n",
        "    data_all['text'] = (\n",
        "        data_all['Title'].fillna('') + ' ' +\n",
        "        data_all['Content'].fillna('') + ' ' +\n",
        "        data_all['Paragraph'].fillna('')\n",
        "    )\n",
        "    return data_all[['text', 'label', 'source', 'topic']].dropna(subset=['text'])"
      ],
      "metadata": {
        "id": "BaQgXwH-GKKd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================\n",
        "#    TEXT STATISTICS AND READABILITY ANALYSIS\n",
        "# ==============================================\n",
        "def calculate_text_stats(text_series):\n",
        "    \"\"\"Calculate comprehensive statistics for Marathi text\"\"\"\n",
        "    stats = []\n",
        "    for text in text_series:\n",
        "        tokens = indic_tokenize.trivial_tokenize(text)\n",
        "        words = [t for t in tokens if t.strip() and not t.isspace()]\n",
        "        sentences = [s for s in text.split('à¥¤') if s.strip()]\n",
        "\n",
        "        # Syllable counting\n",
        "        syllable_counts = []\n",
        "        polysyllabic_words = 0\n",
        "        for word in words:\n",
        "            syllables = max(1, len(re.findall(r'[\\u0900-\\u097F\\u0951\\u0952]', word)))\n",
        "            syllable_counts.append(syllables)\n",
        "            if syllables > 2:\n",
        "                polysyllabic_words += 1\n",
        "\n",
        "        stats.append({\n",
        "            'num_tokens': len(tokens),\n",
        "            'num_words': len(words),\n",
        "            'num_unique_words': len(set(words)),\n",
        "            'num_sentences': len(sentences),\n",
        "            'avg_word_length': np.mean([len(w) for w in words]) if words else 0,\n",
        "            'avg_sentence_length': len(words)/len(sentences) if sentences else 0,\n",
        "            'avg_syllables_per_word': np.mean(syllable_counts) if syllable_counts else 0,\n",
        "            'polysyllabic_words': polysyllabic_words\n",
        "        })\n",
        "    return pd.DataFrame(stats)\n",
        "\n",
        "def compute_readability_scores(stats_df):\n",
        "    \"\"\"Calculate Marathi Readability Score (MRS)\"\"\"\n",
        "    stats_df['mrs'] = -2.34 + 2.14 * stats_df['avg_word_length'] + 0.01 * stats_df['polysyllabic_words']\n",
        "    return stats_df\n",
        "\n",
        "def generate_text_statistics_table(original_texts, processed_texts):\n",
        "    \"\"\"Generate Table I comparison with proper labels\"\"\"\n",
        "    # Calculate statistics\n",
        "    original_stats = calculate_text_stats(original_texts)\n",
        "    processed_stats = calculate_text_stats(processed_texts)\n",
        "\n",
        "    # Create DataFrames with proper index names\n",
        "    original_df = pd.DataFrame({\n",
        "        \"# Tokens\": [\n",
        "            original_stats[\"num_tokens\"].max(),\n",
        "            original_stats[\"num_tokens\"].min(),\n",
        "            original_stats[\"num_tokens\"].mean()\n",
        "        ],\n",
        "        \"# Words\": [\n",
        "            original_stats[\"num_words\"].max(),\n",
        "            original_stats[\"num_words\"].min(),\n",
        "            original_stats[\"num_words\"].mean()\n",
        "        ],\n",
        "        \"# Unique Words\": [\n",
        "            original_stats[\"num_unique_words\"].max(),\n",
        "            original_stats[\"num_unique_words\"].min(),\n",
        "            original_stats[\"num_unique_words\"].mean()\n",
        "        ],\n",
        "        \"# Sentences\": [\n",
        "            original_stats[\"num_sentences\"].max(),\n",
        "            original_stats[\"num_sentences\"].min(),\n",
        "            original_stats[\"num_sentences\"].mean()\n",
        "        ]\n",
        "    }, index=[\"Max\", \"Min\", \"Average\"])\n",
        "\n",
        "    processed_df = pd.DataFrame({\n",
        "        \"# Tokens\": [\n",
        "            processed_stats[\"num_tokens\"].max(),\n",
        "            processed_stats[\"num_tokens\"].min(),\n",
        "            processed_stats[\"num_tokens\"].mean()\n",
        "        ],\n",
        "        \"# Words\": [\n",
        "            processed_stats[\"num_words\"].max(),\n",
        "            processed_stats[\"num_words\"].min(),\n",
        "            processed_stats[\"num_words\"].mean()\n",
        "        ],\n",
        "        \"# Unique Words\": [\n",
        "            processed_stats[\"num_unique_words\"].max(),\n",
        "            processed_stats[\"num_unique_words\"].min(),\n",
        "            processed_stats[\"num_unique_words\"].mean()\n",
        "        ],\n",
        "        \"# Sentences\": [\n",
        "            processed_stats[\"num_sentences\"].max(),\n",
        "            processed_stats[\"num_sentences\"].min(),\n",
        "            processed_stats[\"num_sentences\"].mean()\n",
        "        ]\n",
        "    }, index=[\"Max\", \"Min\", \"Average\"])\n",
        "\n",
        "    # Create final table with section headers\n",
        "    final_table = pd.concat({\n",
        "        \"Original Articles\": original_df,\n",
        "        \"Processed Articles\": processed_df\n",
        "    }, names=['Article Type', 'Metric'])\n",
        "\n",
        "    # Format display\n",
        "    print(\"TABLE I\")\n",
        "    print(\"\\nTHE TABLE SHOWS THE MAXIMUM, MINIMUM, AND AVERAGE VALUES FOR TOKENS, WORDS, UNIQUE WORDS, AND SENTENCES\\n\")\n",
        "\n",
        "    return final_table.reset_index(level='Article Type')\n",
        "\n",
        "def generate_complete_readability_table(df, text_col='text', label_col='label',\n",
        "                                     source_col='source', topic_col='topic'):\n",
        "    \"\"\"Generate three separate tables for readability scores\"\"\"\n",
        "    # Calculate statistics\n",
        "    stats_df = calculate_text_stats(df[text_col])\n",
        "    df_with_mrs = df.copy()\n",
        "    df_with_mrs['mrs'] = compute_readability_scores(stats_df)['mrs']\n",
        "\n",
        "    # Build comparison data\n",
        "    source_mrs = df_with_mrs.groupby(source_col)['mrs'].mean().sort_values(ascending=False)\n",
        "    topic_comparison = df_with_mrs.pivot_table(index=topic_col, columns=label_col,\n",
        "                                            values='mrs', aggfunc='mean')\n",
        "    class_mrs = df_with_mrs.groupby(label_col)['mrs'].mean()\n",
        "\n",
        "    # =====================================\n",
        "    # Table 1: Source-wise Scores\n",
        "    # =====================================\n",
        "    source_table = pd.DataFrame({\n",
        "        'Source': source_mrs.index,\n",
        "        'MRS Score': [round(score, 2) for score in source_mrs.values]\n",
        "    })\n",
        "\n",
        "    # =====================================\n",
        "    # Table 2: Topic-wise Comparison\n",
        "    # =====================================\n",
        "    topic_table = pd.DataFrame({\n",
        "        'Topic': topic_comparison.index,\n",
        "        'False (HRS)': [round(score, 2) for score in topic_comparison[0].values],\n",
        "        'True (HRS)': [round(score, 2) for score in topic_comparison[1].values]\n",
        "    })\n",
        "\n",
        "    # =====================================\n",
        "    # Table 3: Class-wise Comparison\n",
        "    # =====================================\n",
        "    class_table = pd.DataFrame({\n",
        "        'Class': ['False', 'True'],\n",
        "        'MRS Score': [round(class_mrs[0], 2), round(class_mrs[1], 2)]\n",
        "    })\n",
        "\n",
        "    # =====================================\n",
        "    #         Display all tables\n",
        "    # =====================================\n",
        "    print(\"\\nTABLE II: MARATHI READABILITY SCORES (MRS) COMPARISON\")\n",
        "\n",
        "    # Common style settings\n",
        "    table_style = [\n",
        "        {'selector': 'th',\n",
        "         'props': [('background-color', '#f2f2f2'), ('text-align', 'center')]},\n",
        "        {'selector': 'td',\n",
        "         'props': [('text-align', 'center')]},\n",
        "        {'selector': '',\n",
        "         'props': [('border', '1px solid black')]}\n",
        "    ]\n",
        "\n",
        "    print(\"\\n1. Source-wise Scores:\")\n",
        "    source_styler = source_table.style.set_table_styles(table_style)\n",
        "    if hasattr(source_styler, 'hide_index'):\n",
        "        source_styler = source_styler.hide_index()\n",
        "    display(source_styler)\n",
        "\n",
        "    print(\"\\n2. Topic-wise Comparison:\")\n",
        "    topic_styler = topic_table.style.set_table_styles(table_style)\n",
        "    if hasattr(topic_styler, 'hide_index'):\n",
        "        topic_styler = topic_styler.hide_index()\n",
        "    display(topic_styler)\n",
        "\n",
        "    print(\"\\n3. Class-wise Comparison:\")\n",
        "    class_styler = class_table.style.set_table_styles(table_style)\n",
        "    if hasattr(class_styler, 'hide_index'):\n",
        "        class_styler = class_styler.hide_index()\n",
        "    display(class_styler)"
      ],
      "metadata": {
        "id": "vaZ8-zi_0Lmn"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "#        MAIN EXECUTION PIPELINE\n",
        "# ======================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize text processor\n",
        "    marathi_processor = MarathiTextProcessor()\n",
        "\n",
        "    # Load and prepare data\n",
        "    positive, negative = load_and_preview_examples()\n",
        "    data_all = prepare_datasets(positive, negative)\n",
        "\n",
        "    # Split data\n",
        "    X = data_all[['text', 'source', 'topic']]\n",
        "    y = data_all['label']\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Preprocess text (MUST HAPPEN FIRST)\n",
        "    print(\"\\nğŸ”„ Preprocessing text...\")\n",
        "    X_train_processed = X_train['text'].apply(marathi_processor.preprocess)\n",
        "    X_test_processed = X_test['text'].apply(marathi_processor.preprocess)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KYhoL8B927Z",
        "outputId": "35304848-c733-4e2f-e250-331e98610c81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Positive Samples: 2223 | Sources: ['Lokmat' 'NDTV']\n",
            "âŒ Negative Samples: 2086 | Sources: ['FactCrescendo']\n",
            "\n",
            "ğŸ”„ Preprocessing text...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================\n",
        "#          GENERATE TABLES\n",
        "# ==================================\n",
        "\n",
        "# Generate Table I\n",
        "print(\"\\nğŸ“Š Text Statistics Comparison\\n\")\n",
        "display(generate_text_statistics_table(X_train['text'], X_train_processed))\n",
        "\n",
        "# Generate Table II\n",
        "print(\"\\nğŸ” Readability Analysis\")\n",
        "generate_complete_readability_table(\n",
        "    X_train.assign(label=y_train),\n",
        "    text_col='text',\n",
        "    label_col='label',\n",
        "    source_col='source',\n",
        "    topic_col='topic'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 980
        },
        "id": "u0dlI2hBB6L1",
        "outputId": "e12f5a15-46e1-4507-d466-56f976255e2f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Text Statistics Comparison\n",
            "\n",
            "TABLE I\n",
            "\n",
            "THE TABLE SHOWS THE MAXIMUM, MINIMUM, AND AVERAGE VALUES FOR TOKENS, WORDS, UNIQUE WORDS, AND SENTENCES\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               Article Type     # Tokens      # Words  # Unique Words  \\\n",
              "Metric                                                                  \n",
              "Max       Original Articles  4972.000000  4971.000000     1939.000000   \n",
              "Min       Original Articles     1.000000     0.000000        0.000000   \n",
              "Average   Original Articles   235.568320   234.651291      149.015376   \n",
              "Max      Processed Articles  3698.000000  3698.000000     1721.000000   \n",
              "Min      Processed Articles     1.000000     0.000000        0.000000   \n",
              "Average  Processed Articles   183.742385   183.736873      127.540180   \n",
              "\n",
              "         # Sentences  \n",
              "Metric                \n",
              "Max       109.000000  \n",
              "Min         0.000000  \n",
              "Average     1.056281  \n",
              "Max       109.000000  \n",
              "Min         0.000000  \n",
              "Average     1.049318  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b21721f-9b58-4c74-a830-8011a246f26b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Article Type</th>\n",
              "      <th># Tokens</th>\n",
              "      <th># Words</th>\n",
              "      <th># Unique Words</th>\n",
              "      <th># Sentences</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Metric</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Max</th>\n",
              "      <td>Original Articles</td>\n",
              "      <td>4972.000000</td>\n",
              "      <td>4971.000000</td>\n",
              "      <td>1939.000000</td>\n",
              "      <td>109.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Min</th>\n",
              "      <td>Original Articles</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Average</th>\n",
              "      <td>Original Articles</td>\n",
              "      <td>235.568320</td>\n",
              "      <td>234.651291</td>\n",
              "      <td>149.015376</td>\n",
              "      <td>1.056281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Max</th>\n",
              "      <td>Processed Articles</td>\n",
              "      <td>3698.000000</td>\n",
              "      <td>3698.000000</td>\n",
              "      <td>1721.000000</td>\n",
              "      <td>109.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Min</th>\n",
              "      <td>Processed Articles</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Average</th>\n",
              "      <td>Processed Articles</td>\n",
              "      <td>183.742385</td>\n",
              "      <td>183.736873</td>\n",
              "      <td>127.540180</td>\n",
              "      <td>1.049318</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b21721f-9b58-4c74-a830-8011a246f26b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0b21721f-9b58-4c74-a830-8011a246f26b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0b21721f-9b58-4c74-a830-8011a246f26b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-1dd5f8df-8647-49a4-970a-be6c69290f4d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1dd5f8df-8647-49a4-970a-be6c69290f4d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-1dd5f8df-8647-49a4-970a-be6c69290f4d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \")\",\n  \"rows\": 6,\n  \"fields\": [\n    {\n      \"column\": \"Metric\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Max\",\n          \"Min\",\n          \"Average\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Article Type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Processed Articles\",\n          \"Original Articles\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"# Tokens\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2223.057443031631,\n        \"min\": 1.0,\n        \"max\": 4972.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          1.0,\n          183.74238468233247\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"# Words\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2223.125196384928,\n        \"min\": 0.0,\n        \"max\": 4971.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          183.73687264287787\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"# Unique Words\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 914.0338220594366,\n        \"min\": 0.0,\n        \"max\": 1939.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.0,\n          127.54017986655063\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"# Sentences\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 56.01750501557267,\n        \"min\": 0.0,\n        \"max\": 109.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.0,\n          1.049318247751668\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ” Readability Analysis\n",
            "\n",
            "TABLE II: MARATHI READABILITY SCORES (MRS) COMPARISON\n",
            "\n",
            "1. Source-wise Scores:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7ce7bc9a31d0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_ef02b th {\n",
              "  background-color: #f2f2f2;\n",
              "  text-align: center;\n",
              "}\n",
              "#T_ef02b td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_ef02b  {\n",
              "  border: 1px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_ef02b\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_ef02b_level0_col0\" class=\"col_heading level0 col0\" >Source</th>\n",
              "      <th id=\"T_ef02b_level0_col1\" class=\"col_heading level0 col1\" >MRS Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_ef02b_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_ef02b_row0_col0\" class=\"data row0 col0\" >Lokmat</td>\n",
              "      <td id=\"T_ef02b_row0_col1\" class=\"data row0 col1\" >10.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ef02b_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_ef02b_row1_col0\" class=\"data row1 col0\" >FactCrescendo</td>\n",
              "      <td id=\"T_ef02b_row1_col1\" class=\"data row1 col1\" >10.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_ef02b_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_ef02b_row2_col0\" class=\"data row2 col0\" >NDTV</td>\n",
              "      <td id=\"T_ef02b_row2_col1\" class=\"data row2 col1\" >10.330000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "2. Topic-wise Comparison:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7ce8648b5c90>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_62cc0 th {\n",
              "  background-color: #f2f2f2;\n",
              "  text-align: center;\n",
              "}\n",
              "#T_62cc0 td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_62cc0  {\n",
              "  border: 1px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_62cc0\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_62cc0_level0_col0\" class=\"col_heading level0 col0\" >Topic</th>\n",
              "      <th id=\"T_62cc0_level0_col1\" class=\"col_heading level0 col1\" >False (HRS)</th>\n",
              "      <th id=\"T_62cc0_level0_col2\" class=\"col_heading level0 col2\" >True (HRS)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_62cc0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_62cc0_row0_col0\" class=\"data row0 col0\" >Health</td>\n",
              "      <td id=\"T_62cc0_row0_col1\" class=\"data row0 col1\" >10.130000</td>\n",
              "      <td id=\"T_62cc0_row0_col2\" class=\"data row0 col2\" >10.510000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_62cc0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_62cc0_row1_col0\" class=\"data row1 col0\" >Others</td>\n",
              "      <td id=\"T_62cc0_row1_col1\" class=\"data row1 col1\" >10.330000</td>\n",
              "      <td id=\"T_62cc0_row1_col2\" class=\"data row1 col2\" >10.340000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_62cc0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
              "      <td id=\"T_62cc0_row2_col0\" class=\"data row2 col0\" >Politics</td>\n",
              "      <td id=\"T_62cc0_row2_col1\" class=\"data row2 col1\" >11.270000</td>\n",
              "      <td id=\"T_62cc0_row2_col2\" class=\"data row2 col2\" >10.340000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_62cc0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
              "      <td id=\"T_62cc0_row3_col0\" class=\"data row3 col0\" >Sports</td>\n",
              "      <td id=\"T_62cc0_row3_col1\" class=\"data row3 col1\" >9.870000</td>\n",
              "      <td id=\"T_62cc0_row3_col2\" class=\"data row3 col2\" >10.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_62cc0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
              "      <td id=\"T_62cc0_row4_col0\" class=\"data row4 col0\" >Technology</td>\n",
              "      <td id=\"T_62cc0_row4_col1\" class=\"data row4 col1\" >10.170000</td>\n",
              "      <td id=\"T_62cc0_row4_col2\" class=\"data row4 col2\" >10.930000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Class-wise Comparison:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7ce7beeb6d50>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_fc659 th {\n",
              "  background-color: #f2f2f2;\n",
              "  text-align: center;\n",
              "}\n",
              "#T_fc659 td {\n",
              "  text-align: center;\n",
              "}\n",
              "#T_fc659  {\n",
              "  border: 1px solid black;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_fc659\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_fc659_level0_col0\" class=\"col_heading level0 col0\" >Class</th>\n",
              "      <th id=\"T_fc659_level0_col1\" class=\"col_heading level0 col1\" >MRS Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_fc659_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
              "      <td id=\"T_fc659_row0_col0\" class=\"data row0 col0\" >False</td>\n",
              "      <td id=\"T_fc659_row0_col1\" class=\"data row0 col1\" >10.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_fc659_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
              "      <td id=\"T_fc659_row1_col0\" class=\"data row1 col0\" >True</td>\n",
              "      <td id=\"T_fc659_row1_col1\" class=\"data row1 col1\" >10.340000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====================================\n",
        "#          Vectorization\n",
        "# ====================================\n",
        "marathi_stop_words = ['à¤µà¤¾à¤šà¤¾ à¤¸à¤¤à¥à¤¯', 'à¤µà¥à¤¹à¤¾à¤¯à¤°à¤²', 'à¤¨à¤¾à¤¹à¥€', 'à¤¸à¥à¤•à¥à¤°à¤¿à¤ªà¥à¤Ÿà¥‡à¤¡', 'à¤¨à¤µà¥à¤¹à¤¤à¤¾','à¤¨à¤µ', 'à¤¯à¤°à¤²', 'à¤¸à¤¤', 'à¤¹à¤¤']\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_features=15000,\n",
        "    ngram_range=(1, 3),\n",
        "    min_df=5,\n",
        "    max_df=0.7,\n",
        "    analyzer='word',\n",
        "    sublinear_tf=True,\n",
        "    stop_words=marathi_stop_words  # Explicitly exclude these words\n",
        ")"
      ],
      "metadata": {
        "id": "1GgSVorspOt4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# Enhanced Classifier Dictionary (Optimized)\n",
        "# ==========================================\n",
        "dict_classifiers = {\n",
        "    \"Logistic Regression\": LogisticRegression(\n",
        "        max_iter=10000,  # Increased for complex Marathi features\n",
        "        class_weight='balanced',\n",
        "        solver='saga',\n",
        "        penalty='elasticnet',\n",
        "        l1_ratio=0.7,    # More L1 regularization for feature selection\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"SVM (Linear)\": LinearSVC(  # Changed to LinearSVC for better efficiency\n",
        "        C=0.5,           # More regularization\n",
        "        class_weight='balanced',\n",
        "        max_iter=10000,\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        n_estimators=300,  # Reduced to prevent overfitting\n",
        "        max_depth=12,\n",
        "        class_weight='balanced_subsample',\n",
        "        min_samples_leaf=10,  # More conservative\n",
        "        n_jobs=-1,       # Parallel processing\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"XGBoost\": XGBClassifier(\n",
        "        n_estimators=250,\n",
        "        max_depth=5,     # Shallower trees\n",
        "        learning_rate=0.05,  # Slower learning\n",
        "        tree_method='hist',\n",
        "        scale_pos_weight=len(y_train[y_train==0])/len(y_train[y_train==1]),\n",
        "        eval_metric='logloss',\n",
        "        random_state=42\n",
        "    ),\n",
        "    \"Naive Bayes\": ComplementNB(\n",
        "        alpha=0.1        # Additive smoothing\n",
        "    ),\n",
        "    \"MLP\": MLPClassifier(\n",
        "        hidden_layer_sizes=(128,64),  # Deeper architecture\n",
        "        early_stopping=True,\n",
        "        validation_fraction=0.15,\n",
        "        alpha=0.001,     # Stronger regularization\n",
        "        batch_size=64,\n",
        "        random_state=42,\n",
        "        max_iter=200\n",
        "    )\n",
        "}"
      ],
      "metadata": {
        "id": "tiDFK8sDSz2O"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================\n",
        "# Enhanced Batch Classifier\n",
        "# ==========================\n",
        "def batch_classify(X_train, Y_train, X_test, Y_test, verbose=True):\n",
        "    dict_models = {}\n",
        "\n",
        "    for classifier_name, classifier in dict_classifiers.items():\n",
        "        try:\n",
        "            t_start = time.perf_counter()\n",
        "\n",
        "            # Handle sparse matrices\n",
        "            if scipy.sparse.issparse(X_train):\n",
        "                X_train_ = X_train.toarray() if X_train.shape[1] <= 50000 else X_train\n",
        "            else:\n",
        "                X_train_ = X_train\n",
        "\n",
        "            if scipy.sparse.issparse(X_test):\n",
        "                X_test_ = X_test.toarray() if X_test.shape[1] <= 50000 else X_test\n",
        "            else:\n",
        "                X_test_ = X_test\n",
        "\n",
        "            # Training\n",
        "            if verbose:\n",
        "                print(f\"ğŸ”„ Training {classifier_name}...\")\n",
        "\n",
        "            classifier.fit(X_train_, Y_train)\n",
        "\n",
        "            # Predictions\n",
        "            y_pred = classifier.predict(X_test_)\n",
        "\n",
        "            # Calculate metrics\n",
        "            cm = confusion_matrix(Y_test, y_pred)\n",
        "            metrics = {\n",
        "                'train_score': classifier.score(X_train_, Y_train),\n",
        "                'test_score': classifier.score(X_test_, Y_test),\n",
        "                'precision': precision_score(Y_test, y_pred, zero_division=0),\n",
        "                'recall': recall_score(Y_test, y_pred),\n",
        "                'f1': f1_score(Y_test, y_pred),\n",
        "                'roc_auc': roc_auc_score(Y_test, y_pred) if len(np.unique(Y_test)) > 1 else 0,\n",
        "                'train_time': time.perf_counter() - t_start,\n",
        "                'confusion_matrix': cm,\n",
        "                'tp': cm[1,1],  # True positives\n",
        "                'fp': cm[0,1],  # False positives\n",
        "                'tn': cm[0,0],  # True negatives\n",
        "                'fn': cm[1,0]   # False negatives\n",
        "            }\n",
        "\n",
        "            dict_models[classifier_name] = {\n",
        "                'model': classifier,\n",
        "                **{k: round(v, 4) if isinstance(v, (int, float)) else v\n",
        "                   for k,v in metrics.items()}\n",
        "            }\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"âœ… {classifier_name:25s} | Test F1: {metrics['f1']:.3f} | AUC: {metrics['roc_auc']:.3f} | Time: {metrics['train_time']:.1f}s\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ {classifier_name:25s} | Failed: {str(e)}\")\n",
        "            continue\n",
        "\n",
        "    return dict_models"
      ],
      "metadata": {
        "id": "GV7Pu6Gc8VfB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "#  Robust Results Display\n",
        "# ========================\n",
        "def display_results(dict_models, sort_by='f1'):\n",
        "    if not dict_models:\n",
        "        print(\"âš ï¸ No models trained successfully!\")\n",
        "        return\n",
        "\n",
        "    # Prepare dataframe with safe metric access\n",
        "    results = []\n",
        "    for name, model_info in dict_models.items():\n",
        "        row = {\n",
        "            'Classifier': name,\n",
        "            'Train Acc': model_info.get('train_score', 0),\n",
        "            'Test Acc': model_info.get('test_score', 0),\n",
        "            'F1': model_info.get('f1', 0),\n",
        "            'Precision': model_info.get('precision', 0),\n",
        "            'Recall': model_info.get('recall', 0),\n",
        "            'AUC': model_info.get('roc_auc', 0),\n",
        "            'TP': model_info.get('tp', 0),\n",
        "            'FP': model_info.get('fp', 0),\n",
        "            'TN': model_info.get('tn', 0),\n",
        "            'FN': model_info.get('fn', 0),\n",
        "            'Time (s)': model_info.get('train_time', 0)\n",
        "        }\n",
        "        results.append(row)\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Ensure sort column exists\n",
        "    if sort_by not in df.columns:\n",
        "        sort_by = 'Test Acc'  # Fallback to accuracy\n",
        "\n",
        "    df = df.sort_values(by=sort_by, ascending=False)\n",
        "\n",
        "    # Style the dataframe\n",
        "    def highlight_max(s):\n",
        "        is_max = s == s.max()\n",
        "        return ['background-color: lightgreen' if v else '' for v in is_max]\n",
        "\n",
        "    styler = df.style\n",
        "    numeric_cols = ['Train Acc', 'Test Acc', 'F1', 'Precision', 'Recall', 'AUC']\n",
        "\n",
        "    # Apply styling only to existing columns\n",
        "    for col in numeric_cols:\n",
        "        if col in df.columns:\n",
        "            styler = styler.apply(highlight_max, subset=[col])\n",
        "            styler = styler.format(\"{:.3f}\", subset=[col])\n",
        "\n",
        "    if 'Time (s)' in df.columns:\n",
        "        styler = styler.background_gradient(cmap='Blues', subset=['Time (s)'])\n",
        "\n",
        "    # Hide index using compatible method\n",
        "    if hasattr(styler, 'hide_index'):\n",
        "        styler = styler.hide_index()\n",
        "    else:\n",
        "        styler = styler.set_table_styles([{\n",
        "            'selector': 'th.row_heading, td.row_heading',\n",
        "            'props': [('display', 'none')]\n",
        "        }])\n",
        "\n",
        "    display(styler)\n",
        "\n",
        "    # Print best model safely\n",
        "    if not df.empty:\n",
        "        best_model = df.iloc[0]\n",
        "        print(f\"\\nğŸ† Best Model: {best_model['Classifier']} ({sort_by}: {best_model[sort_by]:.3f})\")"
      ],
      "metadata": {
        "id": "aQsEVtDQAgDt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "#     Robust Execution\n",
        "# ========================\n",
        "print(\"ğŸ” Vectorizing Marathi text...\")\n",
        "try:\n",
        "    X_train_vec = vectorizer.fit_transform(X_train_processed)\n",
        "    X_test_vec = vectorizer.transform(X_test_processed)\n",
        "\n",
        "    print(\"\\nğŸ§ª Evaluating classifiers...\")\n",
        "    dict_models = batch_classify(X_train_vec, y_train, X_test_vec, y_test)\n",
        "\n",
        "    if dict_models:\n",
        "        print(\"\\nğŸ“Š Results Summary:\")\n",
        "        display_results(dict_models)\n",
        "\n",
        "        # Safely get best model\n",
        "        valid_models = {k:v for k,v in dict_models.items() if 'f1' in v}\n",
        "        if valid_models:\n",
        "            best_model_name = max(valid_models, key=lambda x: valid_models[x]['f1'])\n",
        "            model_path = f\"best_marathi_model_{best_model_name.replace(' ', '_').lower()}.pkl\"\n",
        "            joblib.dump({\n",
        "                'model': valid_models[best_model_name]['model'],\n",
        "                'vectorizer': vectorizer,\n",
        "                'metrics': {k:v for k,v in valid_models[best_model_name].items() if k != 'model'}\n",
        "            }, model_path)\n",
        "            print(f\"\\nğŸ’¾ Saved best model to: {model_path}\")\n",
        "        else:\n",
        "            print(\"\\nâš ï¸ No models with valid F1 scores\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸ All classifiers failed! Check error messages above.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nğŸ”¥ Critical error in pipeline: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        },
        "id": "NAYr5nXM8gi0",
        "outputId": "9a8ed192-32d2-4a72-ba91-c32ea28a7535"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Vectorizing Marathi text...\n",
            "\n",
            "ğŸ§ª Evaluating classifiers...\n",
            "ğŸ”„ Training Logistic Regression...\n",
            "âœ… Logistic Regression       | Test F1: 0.981 | AUC: 0.978 | Time: 19.0s\n",
            "ğŸ”„ Training SVM (Linear)...\n",
            "âœ… SVM (Linear)              | Test F1: 0.993 | AUC: 0.993 | Time: 0.6s\n",
            "ğŸ”„ Training Random Forest...\n",
            "âœ… Random Forest             | Test F1: 0.990 | AUC: 0.990 | Time: 8.9s\n",
            "ğŸ”„ Training XGBoost...\n",
            "âœ… XGBoost                   | Test F1: 0.995 | AUC: 0.995 | Time: 88.6s\n",
            "ğŸ”„ Training Naive Bayes...\n",
            "âœ… Naive Bayes               | Test F1: 0.978 | AUC: 0.974 | Time: 1.2s\n",
            "ğŸ”„ Training MLP...\n",
            "âœ… MLP                       | Test F1: 0.995 | AUC: 0.994 | Time: 29.4s\n",
            "\n",
            "ğŸ“Š Results Summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7ce7bc956410>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "#T_9af3e th.row_heading {\n",
              "  display: none;\n",
              "}\n",
              "#T_9af3e  td.row_heading {\n",
              "  display: none;\n",
              "}\n",
              "#T_9af3e_row0_col1, #T_9af3e_row0_col2, #T_9af3e_row0_col4, #T_9af3e_row0_col6, #T_9af3e_row1_col2, #T_9af3e_row1_col3, #T_9af3e_row2_col1, #T_9af3e_row5_col5 {\n",
              "  background-color: lightgreen;\n",
              "}\n",
              "#T_9af3e_row0_col11 {\n",
              "  background-color: #08306b;\n",
              "  color: #f1f1f1;\n",
              "}\n",
              "#T_9af3e_row1_col11 {\n",
              "  background-color: #aed1e7;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_9af3e_row2_col11 {\n",
              "  background-color: #f7fbff;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_9af3e_row3_col11 {\n",
              "  background-color: #e4eff9;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_9af3e_row4_col11 {\n",
              "  background-color: #cee0f2;\n",
              "  color: #000000;\n",
              "}\n",
              "#T_9af3e_row5_col11 {\n",
              "  background-color: #f6faff;\n",
              "  color: #000000;\n",
              "}\n",
              "</style>\n",
              "<table id=\"T_9af3e\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th class=\"blank level0\" >&nbsp;</th>\n",
              "      <th id=\"T_9af3e_level0_col0\" class=\"col_heading level0 col0\" >Classifier</th>\n",
              "      <th id=\"T_9af3e_level0_col1\" class=\"col_heading level0 col1\" >Train Acc</th>\n",
              "      <th id=\"T_9af3e_level0_col2\" class=\"col_heading level0 col2\" >Test Acc</th>\n",
              "      <th id=\"T_9af3e_level0_col3\" class=\"col_heading level0 col3\" >F1</th>\n",
              "      <th id=\"T_9af3e_level0_col4\" class=\"col_heading level0 col4\" >Precision</th>\n",
              "      <th id=\"T_9af3e_level0_col5\" class=\"col_heading level0 col5\" >Recall</th>\n",
              "      <th id=\"T_9af3e_level0_col6\" class=\"col_heading level0 col6\" >AUC</th>\n",
              "      <th id=\"T_9af3e_level0_col7\" class=\"col_heading level0 col7\" >TP</th>\n",
              "      <th id=\"T_9af3e_level0_col8\" class=\"col_heading level0 col8\" >FP</th>\n",
              "      <th id=\"T_9af3e_level0_col9\" class=\"col_heading level0 col9\" >TN</th>\n",
              "      <th id=\"T_9af3e_level0_col10\" class=\"col_heading level0 col10\" >FN</th>\n",
              "      <th id=\"T_9af3e_level0_col11\" class=\"col_heading level0 col11\" >Time (s)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th id=\"T_9af3e_level0_row0\" class=\"row_heading level0 row0\" >3</th>\n",
              "      <td id=\"T_9af3e_row0_col0\" class=\"data row0 col0\" >XGBoost</td>\n",
              "      <td id=\"T_9af3e_row0_col1\" class=\"data row0 col1\" >1.000</td>\n",
              "      <td id=\"T_9af3e_row0_col2\" class=\"data row0 col2\" >0.994</td>\n",
              "      <td id=\"T_9af3e_row0_col3\" class=\"data row0 col3\" >0.995</td>\n",
              "      <td id=\"T_9af3e_row0_col4\" class=\"data row0 col4\" >1.000</td>\n",
              "      <td id=\"T_9af3e_row0_col5\" class=\"data row0 col5\" >0.989</td>\n",
              "      <td id=\"T_9af3e_row0_col6\" class=\"data row0 col6\" >0.995</td>\n",
              "      <td id=\"T_9af3e_row0_col7\" class=\"data row0 col7\" >454</td>\n",
              "      <td id=\"T_9af3e_row0_col8\" class=\"data row0 col8\" >0</td>\n",
              "      <td id=\"T_9af3e_row0_col9\" class=\"data row0 col9\" >403</td>\n",
              "      <td id=\"T_9af3e_row0_col10\" class=\"data row0 col10\" >5</td>\n",
              "      <td id=\"T_9af3e_row0_col11\" class=\"data row0 col11\" >88.558100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9af3e_level0_row1\" class=\"row_heading level0 row1\" >5</th>\n",
              "      <td id=\"T_9af3e_row1_col0\" class=\"data row1 col0\" >MLP</td>\n",
              "      <td id=\"T_9af3e_row1_col1\" class=\"data row1 col1\" >0.998</td>\n",
              "      <td id=\"T_9af3e_row1_col2\" class=\"data row1 col2\" >0.994</td>\n",
              "      <td id=\"T_9af3e_row1_col3\" class=\"data row1 col3\" >0.995</td>\n",
              "      <td id=\"T_9af3e_row1_col4\" class=\"data row1 col4\" >0.994</td>\n",
              "      <td id=\"T_9af3e_row1_col5\" class=\"data row1 col5\" >0.996</td>\n",
              "      <td id=\"T_9af3e_row1_col6\" class=\"data row1 col6\" >0.994</td>\n",
              "      <td id=\"T_9af3e_row1_col7\" class=\"data row1 col7\" >457</td>\n",
              "      <td id=\"T_9af3e_row1_col8\" class=\"data row1 col8\" >3</td>\n",
              "      <td id=\"T_9af3e_row1_col9\" class=\"data row1 col9\" >400</td>\n",
              "      <td id=\"T_9af3e_row1_col10\" class=\"data row1 col10\" >2</td>\n",
              "      <td id=\"T_9af3e_row1_col11\" class=\"data row1 col11\" >29.417700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9af3e_level0_row2\" class=\"row_heading level0 row2\" >1</th>\n",
              "      <td id=\"T_9af3e_row2_col0\" class=\"data row2 col0\" >SVM (Linear)</td>\n",
              "      <td id=\"T_9af3e_row2_col1\" class=\"data row2 col1\" >1.000</td>\n",
              "      <td id=\"T_9af3e_row2_col2\" class=\"data row2 col2\" >0.993</td>\n",
              "      <td id=\"T_9af3e_row2_col3\" class=\"data row2 col3\" >0.994</td>\n",
              "      <td id=\"T_9af3e_row2_col4\" class=\"data row2 col4\" >0.991</td>\n",
              "      <td id=\"T_9af3e_row2_col5\" class=\"data row2 col5\" >0.996</td>\n",
              "      <td id=\"T_9af3e_row2_col6\" class=\"data row2 col6\" >0.993</td>\n",
              "      <td id=\"T_9af3e_row2_col7\" class=\"data row2 col7\" >457</td>\n",
              "      <td id=\"T_9af3e_row2_col8\" class=\"data row2 col8\" >4</td>\n",
              "      <td id=\"T_9af3e_row2_col9\" class=\"data row2 col9\" >399</td>\n",
              "      <td id=\"T_9af3e_row2_col10\" class=\"data row2 col10\" >2</td>\n",
              "      <td id=\"T_9af3e_row2_col11\" class=\"data row2 col11\" >0.605500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9af3e_level0_row3\" class=\"row_heading level0 row3\" >2</th>\n",
              "      <td id=\"T_9af3e_row3_col0\" class=\"data row3 col0\" >Random Forest</td>\n",
              "      <td id=\"T_9af3e_row3_col1\" class=\"data row3 col1\" >0.994</td>\n",
              "      <td id=\"T_9af3e_row3_col2\" class=\"data row3 col2\" >0.990</td>\n",
              "      <td id=\"T_9af3e_row3_col3\" class=\"data row3 col3\" >0.990</td>\n",
              "      <td id=\"T_9af3e_row3_col4\" class=\"data row3 col4\" >0.998</td>\n",
              "      <td id=\"T_9af3e_row3_col5\" class=\"data row3 col5\" >0.983</td>\n",
              "      <td id=\"T_9af3e_row3_col6\" class=\"data row3 col6\" >0.990</td>\n",
              "      <td id=\"T_9af3e_row3_col7\" class=\"data row3 col7\" >451</td>\n",
              "      <td id=\"T_9af3e_row3_col8\" class=\"data row3 col8\" >1</td>\n",
              "      <td id=\"T_9af3e_row3_col9\" class=\"data row3 col9\" >402</td>\n",
              "      <td id=\"T_9af3e_row3_col10\" class=\"data row3 col10\" >8</td>\n",
              "      <td id=\"T_9af3e_row3_col11\" class=\"data row3 col11\" >8.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9af3e_level0_row4\" class=\"row_heading level0 row4\" >0</th>\n",
              "      <td id=\"T_9af3e_row4_col0\" class=\"data row4 col0\" >Logistic Regression</td>\n",
              "      <td id=\"T_9af3e_row4_col1\" class=\"data row4 col1\" >0.985</td>\n",
              "      <td id=\"T_9af3e_row4_col2\" class=\"data row4 col2\" >0.979</td>\n",
              "      <td id=\"T_9af3e_row4_col3\" class=\"data row4 col3\" >0.981</td>\n",
              "      <td id=\"T_9af3e_row4_col4\" class=\"data row4 col4\" >0.968</td>\n",
              "      <td id=\"T_9af3e_row4_col5\" class=\"data row4 col5\" >0.994</td>\n",
              "      <td id=\"T_9af3e_row4_col6\" class=\"data row4 col6\" >0.978</td>\n",
              "      <td id=\"T_9af3e_row4_col7\" class=\"data row4 col7\" >456</td>\n",
              "      <td id=\"T_9af3e_row4_col8\" class=\"data row4 col8\" >15</td>\n",
              "      <td id=\"T_9af3e_row4_col9\" class=\"data row4 col9\" >388</td>\n",
              "      <td id=\"T_9af3e_row4_col10\" class=\"data row4 col10\" >3</td>\n",
              "      <td id=\"T_9af3e_row4_col11\" class=\"data row4 col11\" >18.978400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th id=\"T_9af3e_level0_row5\" class=\"row_heading level0 row5\" >4</th>\n",
              "      <td id=\"T_9af3e_row5_col0\" class=\"data row5 col0\" >Naive Bayes</td>\n",
              "      <td id=\"T_9af3e_row5_col1\" class=\"data row5 col1\" >0.991</td>\n",
              "      <td id=\"T_9af3e_row5_col2\" class=\"data row5 col2\" >0.976</td>\n",
              "      <td id=\"T_9af3e_row5_col3\" class=\"data row5 col3\" >0.978</td>\n",
              "      <td id=\"T_9af3e_row5_col4\" class=\"data row5 col4\" >0.956</td>\n",
              "      <td id=\"T_9af3e_row5_col5\" class=\"data row5 col5\" >1.000</td>\n",
              "      <td id=\"T_9af3e_row5_col6\" class=\"data row5 col6\" >0.974</td>\n",
              "      <td id=\"T_9af3e_row5_col7\" class=\"data row5 col7\" >459</td>\n",
              "      <td id=\"T_9af3e_row5_col8\" class=\"data row5 col8\" >21</td>\n",
              "      <td id=\"T_9af3e_row5_col9\" class=\"data row5 col9\" >382</td>\n",
              "      <td id=\"T_9af3e_row5_col10\" class=\"data row5 col10\" >0</td>\n",
              "      <td id=\"T_9af3e_row5_col11\" class=\"data row5 col11\" >1.235200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ† Best Model: XGBoost (Test Acc: 0.994)\n",
            "\n",
            "ğŸ’¾ Saved best model to: best_marathi_model_mlp.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================\n",
        "#  TensorFlow Data Preparation (Updated)\n",
        "# =======================================\n",
        "def prepare_tensorflow_data(texts, labels, max_len=100, test_size=0.2):\n",
        "    \"\"\"Prepare data for TensorFlow models with stop word filtering\"\"\"\n",
        "    # Custom stop words to exclude\n",
        "    marathi_stop_words = ['à¤µà¤¾à¤šà¤¾ à¤¸à¤¤à¥à¤¯', 'à¤µà¥à¤¹à¤¾à¤¯à¤°à¤²', 'à¤¨à¤¾à¤¹à¥€', 'à¤¸à¥à¤•à¥à¤°à¤¿à¤ªà¥à¤Ÿà¥‡à¤¡', 'à¤¨à¤µà¥à¤¹à¤¤à¤¾','à¤¨à¤µ', 'à¤¯à¤°à¤²', 'à¤¸à¤¤', 'à¤¹à¤¤']\n",
        "\n",
        "    # Step 1: Filter stop words\n",
        "    filtered_texts = [\n",
        "        ' '.join([word for word in str(text).split()\n",
        "        if word not in marathi_stop_words])\n",
        "        for text in texts\n",
        "    ]\n",
        "\n",
        "    # Step 2: Tokenization\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
        "    tokenizer.fit_on_texts(filtered_texts)\n",
        "    sequences = tokenizer.texts_to_sequences(filtered_texts)\n",
        "\n",
        "    # Step 3: Padding\n",
        "    X = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "    y = np.array(labels)\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=42\n",
        "    )\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test), tokenizer  # Now returns tokenizer\n"
      ],
      "metadata": {
        "id": "l5DJbbpJI8It"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================\n",
        "#            Main Execution\n",
        "# ======================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize text processor\n",
        "    marathi_processor = MarathiTextProcessor()\n",
        "\n",
        "    # Load and prepare data\n",
        "    positive, negative = load_and_preview_examples()\n",
        "    data_all = prepare_datasets(positive, negative)\n",
        "\n",
        "    # TensorFlow Pipeline\n",
        "    print(\"\\nğŸ§  Preparing TensorFlow data...\")\n",
        "    (X_train_tf, y_train_tf), (X_test_tf, y_test_tf), tokenizer = prepare_tensorflow_data(\n",
        "        data_all['text'],\n",
        "        data_all['label']\n",
        "    )\n",
        "\n",
        "    # Verify tokenizer\n",
        "    print(\"\\nâœ… Tokenizer Summary:\")\n",
        "    print(\"Vocabulary size:\", len(tokenizer.word_index))\n",
        "    print(\"Sample words:\", list(tokenizer.word_index.items())[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSoTfVO1qYGx",
        "outputId": "b248c63c-a199-4def-ccf8-ee66e7fae4b5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Positive Samples: 2223 | Sources: ['Lokmat' 'NDTV']\n",
            "âŒ Negative Samples: 2086 | Sources: ['FactCrescendo']\n",
            "\n",
            "ğŸ§  Preparing TensorFlow data...\n",
            "\n",
            "âœ… Tokenizer Summary:\n",
            "Vocabulary size: 89388\n",
            "Sample words: [('à¤†à¤¹à¥‡', 1), ('à¤¯à¤¾', 2), ('à¤†à¤£à¤¿', 3), ('à¤†à¤¹à¥‡à¤¤', 4), ('à¤•à¥‡à¤²à¥€', 5)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # ======================================\n",
        "    #     TensorFlow Model Construction\n",
        "    # ======================================\n",
        "    def build_marathi_text_model(vocab_size=10000, max_len=100, embedding_dim=128):\n",
        "        \"\"\"Build a BiLSTM model for Marathi text classification\"\"\"\n",
        "        # Input layer\n",
        "        input_layer = Input(shape=(max_len,))\n",
        "\n",
        "        # Embedding layer with pretrained weights option\n",
        "        embedding = Embedding(\n",
        "            input_dim=vocab_size,\n",
        "            output_dim=embedding_dim,\n",
        "            input_length=max_len,\n",
        "            mask_zero=True\n",
        "        )(input_layer)\n",
        "\n",
        "        # BiLSTM layers\n",
        "        bilstm1 = Bidirectional(LSTM(\n",
        "            128,\n",
        "            return_sequences=True,\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2\n",
        "        ))(embedding)\n",
        "\n",
        "        bilstm2 = Bidirectional(LSTM(\n",
        "            64,\n",
        "            dropout=0.2,\n",
        "            recurrent_dropout=0.2\n",
        "        ))(bilstm1)\n",
        "\n",
        "        # Dense layers\n",
        "        dense1 = Dense(64, activation='relu')(bilstm2)\n",
        "        dropout1 = Dropout(0.3)(dense1)\n",
        "        dense2 = Dense(32, activation='relu')(dropout1)\n",
        "        output = Dense(1, activation='sigmoid')(dense2)\n",
        "\n",
        "        # Create model\n",
        "        model = Model(inputs=input_layer, outputs=output)\n",
        "\n",
        "        # Compile model\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Build and show model summary\n",
        "    model = build_marathi_text_model(\n",
        "        vocab_size=len(tokenizer.word_index) + 1,  # Now tokenizer is defined\n",
        "        max_len=100\n",
        "    )\n",
        "    model.summary()"
      ],
      "metadata": {
        "id": "bct-5G3uKLPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b41da5a-eca3-400b-ac17-a2b85672997f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 100)]             0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, 100, 128)          11441792  \n",
            "                                                                 \n",
            " bidirectional (Bidirection  (None, 100, 256)          263168    \n",
            " al)                                                             \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirecti  (None, 128)               164352    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 11879681 (45.32 MB)\n",
            "Trainable params: 11879681 (45.32 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # ======================================\n",
        "    #             Model Training\n",
        "    # ======================================\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=3,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_tf, y_train_tf,\n",
        "        validation_data=(X_test_tf, y_test_tf),\n",
        "        epochs=20,\n",
        "        batch_size=64,\n",
        "        callbacks=[early_stopping],\n",
        "        verbose=1\n",
        "    )"
      ],
      "metadata": {
        "id": "IvI5SPpqKY5f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37e568c4-fa57-4a22-be6d-8da03a546a23"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "54/54 [==============================] - 67s 1s/step - loss: 0.2113 - accuracy: 0.9408 - precision: 0.9615 - recall: 0.9212 - val_loss: 0.0295 - val_accuracy: 0.9965 - val_precision: 0.9957 - val_recall: 0.9978\n",
            "Epoch 2/20\n",
            "54/54 [==============================] - 54s 992ms/step - loss: 0.0304 - accuracy: 0.9919 - precision: 0.9989 - recall: 0.9853 - val_loss: 0.0245 - val_accuracy: 0.9977 - val_precision: 0.9978 - val_recall: 0.9978\n",
            "Epoch 3/20\n",
            "54/54 [==============================] - 54s 1s/step - loss: 0.0152 - accuracy: 0.9965 - precision: 0.9955 - recall: 0.9977 - val_loss: 0.0036 - val_accuracy: 0.9988 - val_precision: 1.0000 - val_recall: 0.9978\n",
            "Epoch 4/20\n",
            "54/54 [==============================] - 54s 996ms/step - loss: 6.8084e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.0038 - val_accuracy: 0.9988 - val_precision: 1.0000 - val_recall: 0.9978\n",
            "Epoch 5/20\n",
            "54/54 [==============================] - 53s 989ms/step - loss: 0.0040 - accuracy: 0.9991 - precision: 1.0000 - recall: 0.9983 - val_loss: 0.0034 - val_accuracy: 0.9988 - val_precision: 1.0000 - val_recall: 0.9978\n",
            "Epoch 6/20\n",
            "54/54 [==============================] - 54s 1s/step - loss: 4.0226e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9988 - val_precision: 1.0000 - val_recall: 0.9978\n",
            "Epoch 7/20\n",
            "54/54 [==============================] - 58s 1s/step - loss: 2.9288e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.0037 - val_accuracy: 0.9988 - val_precision: 1.0000 - val_recall: 0.9978\n",
            "Epoch 8/20\n",
            "54/54 [==============================] - 56s 1s/step - loss: 2.6025e-04 - accuracy: 1.0000 - precision: 1.0000 - recall: 1.0000 - val_loss: 0.0036 - val_accuracy: 0.9988 - val_precision: 1.0000 - val_recall: 0.9978\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # ======================================\n",
        "    #           Model Evaluation\n",
        "    # ======================================\n",
        "    def evaluate_model(model, X_test, y_test):\n",
        "        \"\"\"Evaluate model performance\"\"\"\n",
        "        results = model.evaluate(X_test, y_test, verbose=0)\n",
        "        metrics = {\n",
        "            'loss': results[0],\n",
        "            'accuracy': results[1],\n",
        "            'precision': results[2],\n",
        "            'recall': results[3],\n",
        "            'f1': 2 * (results[2] * results[3]) / (results[2] + results[3] + 1e-7)\n",
        "        }\n",
        "        return metrics\n",
        "\n",
        "    metrics = evaluate_model(model, X_test_tf, y_test_tf)\n",
        "    print(\"\\nğŸ“Š Model Evaluation:\")\n",
        "    for name, value in metrics.items():\n",
        "        print(f\"{name.capitalize():<10}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGyVWi6PuOWp",
        "outputId": "573003e5-3558-415f-87a7-14b6d07ab8b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“Š Model Evaluation:\n",
            "Loss      : 0.0034\n",
            "Accuracy  : 0.9988\n",
            "Precision : 1.0000\n",
            "Recall    : 0.9978\n",
            "F1        : 0.9989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "#           PREDICTION FUNCTION (First!)\n",
        "# =============================================\n",
        "def predict_article(article_text, model, tokenizer, max_len=100):\n",
        "    \"\"\"\n",
        "    Predict if a Marathi article is real (1) or fake (0)\n",
        "\n",
        "    Args:\n",
        "        article_text: Raw Marathi text to classify\n",
        "        model: Your trained TensorFlow model\n",
        "        tokenizer: The tokenizer used during training\n",
        "        max_len: Same length used during training (default 100)\n",
        "    \"\"\"\n",
        "    # Initialize processor (must match your training setup)\n",
        "    processor = MarathiTextProcessor()\n",
        "\n",
        "    # 1. Preprocess exactly like training data\n",
        "    processed_text = processor.preprocess(article_text)\n",
        "\n",
        "    # 2. Tokenize and pad\n",
        "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
        "    padded = pad_sequences(sequence, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "    # 3. Predict\n",
        "    proba = model.predict(padded, verbose=0)[0][0]\n",
        "    prediction = 1 if proba >= 0.5 else 0\n",
        "\n",
        "    # 4. Return human-readable results\n",
        "    return {\n",
        "        'prediction': prediction,\n",
        "        'probability': float(proba),\n",
        "        'confidence': f\"{max(proba, 1-proba)*100:.1f}%\",\n",
        "        'interpretation': \"Real/Propaganda\" if prediction == 1 else \"Fake/Fact-checked\",\n",
        "        'processed_text': processed_text  # For debugging\n",
        "    }\n",
        "\n",
        "# =============================================\n",
        "#          INTERACTIVE TEST INTERFACE\n",
        "# =============================================\n",
        "def test_articles_interactively(model, tokenizer):\n",
        "    \"\"\"Test your model with custom articles\"\"\"\n",
        "    from IPython.display import clear_output\n",
        "\n",
        "    print(\"âœ… Model Testing Ready (Type 'exit' to quit)\")\n",
        "    while True:\n",
        "        article = input(\"\\nğŸ“ Paste Marathi article:\\n\")\n",
        "        if article.lower() == 'exit':\n",
        "            break\n",
        "\n",
        "        result = predict_article(article, model, tokenizer)\n",
        "\n",
        "        # Display results\n",
        "        clear_output(wait=True)\n",
        "        print(\"ğŸ” Prediction Results:\")\n",
        "        print(f\"â€¢ Classification: {result['interpretation']} ({result['prediction']})\")\n",
        "        print(f\"â€¢ Confidence: {result['confidence']}\")\n",
        "        print(f\"â€¢ Probability: {result['probability']:.4f}\")\n",
        "        print(\"\\nğŸ› ï¸ Processed Text (Debug):\")\n",
        "        print(result['processed_text'][:200] + \"...\")  # Show first 200 chars\n",
        "\n",
        "# =============================================\n",
        "#               RUN THE TESTER\n",
        "# =============================================\n",
        "# Make sure these variables exist (from your training)\n",
        "print(\"Checking dependencies...\")\n",
        "assert 'model' in globals(), \"Model not found - train your model first!\"\n",
        "assert 'tokenizer' in globals(), \"Tokenizer not found - run data preparation first!\"\n",
        "\n",
        "# Start testing\n",
        "test_articles_interactively(model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L44a1_9d-Ufv",
        "outputId": "24b6ad5b-f119-46bd-b12a-95d157729a15"
      },
      "execution_count": 24,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ” Prediction Results:\n",
            "â€¢ Classification: Fake/Fact-checked (0)\n",
            "â€¢ Confidence: 99.2%\n",
            "â€¢ Probability: 0.0076\n",
            "\n",
            "ğŸ› ï¸ Processed Text (Debug):\n",
            "à¤•à¥à¤‚à¤­à¤®à¥‡à¤³à¥à¤¯à¤¾à¤¤ à¤²à¤·à¥à¤•à¤°à¤¾à¤šà¥à¤¯à¤¾ à¤œà¤µà¤¾à¤¨à¤¾à¤‚à¤µà¤° à¤šà¤ªà¥à¤ªà¤²à¤«à¥‡à¤• à¤¨à¥‡à¤Ÿà¤•à¤°à¤¼à¥à¤¯à¤¾à¤‚à¤¨à¥€ à¤¤à¥à¤«à¤¾à¤¨ à¤¶à¥‡à¤…à¤° à¤•à¥‡à¤²à¤¾ à¤ªà¤£ à¤¨à¥‡à¤®à¤•à¤‚ à¤˜à¤¡à¤²à¤‚ à¤•à¤¾à¤¯ à¤µà¤¾à¤šà¤¾ à¤¸à¤¤à¥à¤¯...\n",
            "\n",
            "ğŸ“ Paste Marathi article:\n",
            "exit\n"
          ]
        }
      ]
    }
  ]
}